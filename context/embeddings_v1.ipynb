{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Create embeddings given a list of links and upload to pinecone.\n",
    "- v1: only wikipedia. Ask a question, give wikipedia titles and we will add context to the question\n",
    "- v2: wikipedia + other sources (depends on the quality of scraper)\n",
    "-- if the scraper is good we can create embeddings for anything on the internet given a link\n",
    "-- we can also search public datasets to get more context\n",
    "-- ideal UI: user posts a bunch of links, asks question, we add context based on those links and answer the question\n",
    "- v3: based on the info we have on what type of context worked for what question, we can then create fine tuned models and then use those to answer questions\n",
    "-- I'm thinking a marketplace where people can use fine tuned gpt's to ask about niche and specific topics like aerospace, finance, etc\n",
    "\n",
    "thx to openai for vv nice cookbooks and examples.\n",
    "\"\"\"\n",
    "\n",
    "# import everything we need\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "# annoying coz conda doesnt list it, so have to install pip in venv and then use the venv's pip to install it\n",
    "import pinecone\n",
    "from ipynb.fs.full.wiki_extract import wiki_extract\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set constants\n",
    "EMBEDDINGS_MODEL = \"text-embedding-ada-002\"\n",
    "EMBEDDINGS_DIMENSION = 1536\n",
    "PINECONE_BATCH_SIZE = 32\n",
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n \"\n",
    "\n",
    "# load env variables\n",
    "load_dotenv()\n",
    "\n",
    "# setup openai and pinecone\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "if openai.api_key is None:\n",
    "    print(\"openai api key not found\")\n",
    "if pinecone is None:\n",
    "    print(\"pinecone api key not found\")\n",
    "\n",
    "# 1000 tokens ~ 750 words; there is no way to get the number of tokens from the API for 2nd gen models for now\n",
    "# 1 token ~ 4 characters\n",
    "def token_estimate(text):\n",
    "    # anything above 8000 tokens is too long for the ada model\n",
    "    return len(text) / 4\n",
    "\n",
    "# we know that openai ada model costs $0.0004 / 1K tokens\n",
    "def cost_estimate(tokens):\n",
    "    return tokens / 1000 * 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if 'openai' index already exists (only create index if not)\n",
    "if 'openai' not in pinecone.list_indexes():\n",
    "    pinecone.create_index('openai', dimension=EMBEDDINGS_DIMENSION)\n",
    "# connect to index\n",
    "index = pinecone.Index('openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai and pinceone stuff\n",
    "\n",
    "# get embeddings for text\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=EMBEDDINGS_MODEL,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# add embeddings to pinecone index\n",
    "def add_to_pinecone(df: pd.DataFrame):\n",
    "    for i in tqdm(range(0, df.shape[0], PINECONE_BATCH_SIZE)):\n",
    "      # set end position of batch\n",
    "      i_end = min(i+PINECONE_BATCH_SIZE, df.shape[0])\n",
    "      # slice df\n",
    "      temp_df = df.loc[i: i_end]\n",
    "      # get batch of lines and IDs\n",
    "      ids_batch = [str(n) for n in range(i, i_end)]\n",
    "      # prep metadata and upsert batch\n",
    "      meta = [{'content': line} for line in temp_df['content'].values]\n",
    "      embeds = temp_df['embeddings'].values\n",
    "      to_upsert = zip(ids_batch, embeds, meta)\n",
    "      # upsert to Pinecone\n",
    "      index.upsert(vectors=list(to_upsert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embeddings and enforce token rules for any df\n",
    "# run this function once your parser has created a df with columns 'title', 'heading', 'content'\n",
    "def get_df_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['tokens'] = df['content'].apply(token_estimate)\n",
    "    # filter tokens by 40-8000\n",
    "    df = df[df['tokens'] > 40]\n",
    "    df = df[df['tokens'] < 8000]\n",
    "    # get embeddings\n",
    "    df['embeddings'] = df['content'].apply(get_embedding)\n",
    "    return df\n",
    "\n",
    "def construct_prompt(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant context for a question, and construct a prompt\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(question)\n",
    "    res = index.query([query_embedding], top_k=5, include_metadata=True)\n",
    "    token_len = 0\n",
    "    header = \"\"\"\\n\\nContext:\\n\"\"\"\n",
    "     \n",
    "    for match in res[\"matches\"]:\n",
    "        # compute token length for match metadata\n",
    "        metadata = match[\"metadata\"][\"content\"]\n",
    "        metadata_len = token_estimate(metadata)\n",
    "        # one for the separator\n",
    "        token_len += metadata_len + 1\n",
    "        if token_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "        header += metadata + SEPARATOR\n",
    "    return header + \"\\n Q: \" + question + \"\\n A:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound cost estimate [0.0038966000000000005]\n"
     ]
    }
   ],
   "source": [
    "# create embeddings for given list of wikipedia pages, am not recursively adding pages since most likely the user will only give a few pages that are beyond the knowledge cutoff date\n",
    "# this is a very simple way to get embeddings, but it works for now\n",
    "\n",
    "input = [\"Ingenuity (helicopter)\", \"List of Ingenuity flights\"]\n",
    "print('upper bound cost estimate', [sum([cost_estimate(token_estimate(wiki.content)) for wiki in wiki_pages])])\n",
    "df = wiki_extract(input)\n",
    "df = get_df_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Context:\n",
      "The NASA helicopter Ingenuity on Mars made the first powered controlled flights by an aircraft on a planet other than Earth. Its first flight was April 19, 2021, after landing February 18 attached to the underside of the Perseverance rover. Ingenuity weighs 1.8 kilograms (4.0 lb) and is 49 cm (19 in) tall. It is powered by six lithium-ion solar-charged batteries. It was built and is operated by the Jet Propulsion Laboratory (JPL), a field center of NASA. It was designed for a 30-day demonstration period, but has operated far above expectations, making its 37th flight 607 days after its first Martian flight.\n",
      " \n",
      " Q: How many flight has the mars helicopter completed?\n",
      " A:\n"
     ]
    }
   ],
   "source": [
    "query = \"How many flight has the mars helicopter completed?\"\n",
    "print(construct_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdf0ee591c73da67f21f68f57a2216acdfa3ab25f9e605c52b032cc59455ae7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
